{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основе данных, полученных на предыдущем этапе работы (Data Engineering), необходимо построить модель для предсказания CTR (Отношение числа кликов к числу просмотров)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходные данные:\n",
    "\n",
    "Файлы - train.parquet, test.parquet, validate.parquet\n",
    "https://github.com/hadjdeh/Stepik_Academy_Big_Data_for_DS/tree/master/ML_Engineering\n",
    "\n",
    "Струтура данных:\n",
    "\n",
    "<b>ad_id</b>\tinteger\tid рекламного объявления\n",
    "\n",
    "<b>target_audience_count</b>\tdecimal\tразмер аудитории, на которую таргетируется \n",
    "\n",
    "<b>has_video</b>integer\t1 если есть видео, иначе 0\n",
    "\n",
    "<b>is_cpm</b>\tinteger\t1 если тип объявления CPM, иначе 0\n",
    "\n",
    "<b>is_cpc</b>\tinteger\t1 если тип объявления CPC, иначе 0\n",
    "\n",
    "<b>ad_cost</b>\tdouble\tстоимость объявления в рублях\n",
    "\n",
    "<b>day_count</b>\tinteger\tЧисло дней, которое показывалась реклама\n",
    "\n",
    "<b>ctr\tdouble</b>\tОтношение числа кликов к числу просмотров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание:\n",
    "    \n",
    "Необходимо реализовать две PySpark задачи:\n",
    "\n",
    "1) <b>PySparkMLFit.py</b> - задача, которая должна тренировать модель на входящих данных, сохранять ее и производить оценку качества модели, используя RegressionEvaluator и выводя в консоль RMSE модели на основе тестового датасета.\n",
    "\n",
    "Варианты запуска задачи:\n",
    "\n",
    "<b>spark-submit PySparkMLFit.py train.parquet test.parquet</b>\n",
    "\n",
    "где:\n",
    "train.parquet - путь к датасету, который необходимо использовать для обучения\n",
    "\n",
    "test.parquet - путь к датасету, который необходимо использовать для оценки полученной модели \n",
    "\n",
    "2) <b>PySparkMLPredict.py</b> - задача, которая должна загружать модель и строить предсказание над переданными ей данными.\n",
    "\n",
    "Варианты запуска задачи:\n",
    "\n",
    "<b>spark-submit PySparkMLPredict.py validate.parquet result</b>\n",
    "\n",
    "где:\n",
    "\n",
    "validate.parquet - путь к датасету, для которого необходимо сделать предсказание\n",
    "\n",
    "result - путь, по которому будет сохранен результат предсказаний в формате CSV следующего вида [ads_id, prediction]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Импортируем необходимые библиотеки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression , DecisionTreeRegressor, RandomForestRegressor,GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import random\n",
    "from pyspark.ml.pipeline import PipelineModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создаем Spark-сессию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"PySparkML\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.137.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1277f2a4e08>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Определяем пути до train, test, validate датасетов и считываем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TRAIN=r'C:\\Users\\hadjd\\stepik-ds-course-master\\Week5\\SparkML\\Project\\train.parquet'\n",
    "PATH_TO_TEST=r'C:\\Users\\hadjd\\stepik-ds-course-master\\Week5\\SparkML\\Project\\test.parquet'\n",
    "PATH_TO_VALIDATE=r'C:\\Users\\hadjd\\stepik-ds-course-master\\Week5\\SparkML\\Project\\validate.parquet'\n",
    "\n",
    "DF_train=spark.read.parquet(PATH_TO_TRAIN)\n",
    "DF_test=spark.read.parquet(PATH_TO_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ad_id: integer (nullable = true)\n",
      " |-- target_audience_count: double (nullable = true)\n",
      " |-- has_video: integer (nullable = true)\n",
      " |-- is_cpm: integer (nullable = true)\n",
      " |-- is_cpc: integer (nullable = true)\n",
      " |-- ad_cost: double (nullable = true)\n",
      " |-- day_count: integer (nullable = true)\n",
      " |-- ctr: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ad_id: integer (nullable = true)\n",
      " |-- target_audience_count: double (nullable = true)\n",
      " |-- has_video: integer (nullable = true)\n",
      " |-- is_cpm: integer (nullable = true)\n",
      " |-- is_cpc: integer (nullable = true)\n",
      " |-- ad_cost: double (nullable = true)\n",
      " |-- day_count: integer (nullable = true)\n",
      " |-- ctr: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99931\n",
      "50016\n"
     ]
    }
   ],
   "source": [
    "print(DF_train.count())\n",
    "print(DF_test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Реализация задачи PySparkMLFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimator_pipeline(train_dataframe, test_dataframe):\n",
    "    \n",
    "    random.seed(0)\n",
    "    \n",
    "    #создаем вектор features из всех колонок, кроме целевой метки ctr\n",
    "    vector=VectorAssembler(inputCols=train_dataframe.columns[:-1], outputCol='features')\n",
    "    \n",
    "    #estimator LR с входными параметрами из задани\n",
    "    estimator_LR = LinearRegression(featuresCol='features',\n",
    "                      labelCol='ctr',\n",
    "                      maxIter=40,\n",
    "                      regParam=0.4,\n",
    "                      elasticNetParam=0.8)\n",
    "    \n",
    "    #другие эстиматоры с параметрами по умолчанию\n",
    "    estimator_DT=DecisionTreeRegressor(featuresCol='features',labelCol='ctr')\n",
    "    estimator_RF=RandomForestRegressor(featuresCol='features', labelCol='ctr')\n",
    "    estimator_GB=GBTRegressor(featuresCol='features', labelCol='ctr')\n",
    "    \n",
    "    #evaluator\n",
    "    RMSE_evaluator=RegressionEvaluator(predictionCol='prediction',labelCol='ctr',metricName='rmse')\n",
    "    \n",
    "    # модели и результаты будем записывать в списки\n",
    "    models=[]\n",
    "    RMSE_result=[]\n",
    "    \n",
    "    #обучаем все эстиматоры\n",
    "    for est_r in [estimator_LR,estimator_DT,estimator_RF,estimator_GB]:\n",
    "        \n",
    "        pipeline=Pipeline(stages=[vector,est_r])\n",
    "        \n",
    "        model=pipeline.fit(train_dataframe)\n",
    "            \n",
    "        models.append(model)\n",
    "        \n",
    "        #делаем прогноз по тестовому датасету\n",
    "        prediction=pipeline.fit(train_dataframe).transform(test_dataframe)\n",
    "        \n",
    "        #считаем метрику RMSE для тестового датасета\n",
    "        RMSE=round(RMSE_evaluator.evaluate(prediction),4)\n",
    "        \n",
    "        #записываем результат в массив для отображения в консоли\n",
    "        RMSE_result.append(RMSE)\n",
    "        \n",
    "        print('Model: {}, \\tRMSE = {}'.format(est_r,RMSE))\n",
    "            \n",
    "    #сохранение моделей в соответствующих директориях\n",
    "    for pair in zip(models,['LR_model','DT_model','RF_model','GB_model']):\n",
    "        pair[0].save(pair[1])\n",
    "    \n",
    "    return models, RMSE_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучаем модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LinearRegression_d8141f38c3ca, \tRMSE = 0.4097\n",
      "Model: DecisionTreeRegressor_56ec07bc42b6, \tRMSE = 0.0899\n",
      "Model: RandomForestRegressor_5c5d8e156fae, \tRMSE = 0.1255\n",
      "Model: GBTRegressor_607ff3eae79c, \tRMSE = 0.0719\n"
     ]
    }
   ],
   "source": [
    "models, results = estimator_pipeline(DF_train,DF_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате, в локальной дирректории создались папки:\n",
    "    \n",
    "- /LR_model\n",
    "- /DT_model\n",
    "- /RF_model\n",
    "- /GB_model\n",
    "\n",
    "В каждую из которых записалась информация о соответстующей модели.\n",
    "\n",
    "Реализация py файла - PySparkMLFit\n",
    "\n",
    "https://github.com/hadjdeh/Stepik_Academy_Big_Data_for_DS/blob/master/ML_Engineering/PySparkMLFit.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Реализация задачи PySparkMLPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_file,output_file):\n",
    "    for model_name in ['LR_model','DT_model','RF_model','GB_model']:\n",
    "        model=PipelineModel.load(model_name)\n",
    "        prediction=model.transform(input_file)\n",
    "        prediction[['ad_id', 'prediction']].coalesce(1).write.option('header', 'true').csv(output_file+'/'+model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Считываем валидационный датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_validate=spark.read.parquet(PATH_TO_VALIDATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Делаем prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(DF_validate,'result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате в локальной директории создалась папка:\n",
    "\n",
    "- /result\n",
    "\n",
    "В которую был записан csv файл с предсказаниями для каждой модели.\n",
    "\n",
    "Реализация PySparkMLPredict.py файла \n",
    "\n",
    "https://github.com/hadjdeh/Stepik_Academy_Big_Data_for_DS/blob/master/ML_Engineering/PySparkMLPredict.py\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
